{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72fc8c0-b21c-47c5-9475-ad26ba72d7f4",
   "metadata": {},
   "source": [
    "# Model and data transfer between servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdcfefd-4d29-4032-bb1f-b9f85a7b3dc4",
   "metadata": {},
   "source": [
    "# Storage space on the GPU server\n",
    "\n",
    "Storage space is limited on the GPU server, so please do not store massive amounts of images in your home folder. The angle90 images are already on there (in /datb/BotsNLE/angle90), so you can simply use that path of create a link (ln -s) to that path to the folder you work in. That way we all use the same files and that saves storage space.\n",
    "\n",
    "If you do wish to use several GB of storage space, please contact Jeroen to facilitate that (we have more storage space on other disks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac135f7-56fa-4872-aa7d-a3d9e4a3f8a0",
   "metadata": {},
   "source": [
    "# Transferring folders from bot to server\n",
    "\n",
    "You can use rsync to transfer an entire folder from the bot to the gpu server. You should use rsync from a Terminal, because you will have to type in your password. For example:\n",
    "\n",
    "`rsync -avr my_images jeroen@gpuserver.hhs.nl:notebooks`\n",
    "\n",
    "will transfer all in the folder `my_images` to the notebooks folder of the user jeroen on the GPU server. You will have to type in the users password to do this. Instead of jeroen, use your own username."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d2a46-05b5-4b32-86e2-ab679c374391",
   "metadata": {},
   "source": [
    "# Saving a learned model\n",
    "\n",
    "The RPi is too slow for training, validation and optimization of model, use the GPU server for that (or your laptop). Once the model is trained you will want to transfer that to the Raspberry Pi to evaluate. The following example assumes that the variable `model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788b530-5cf8-4365-954d-4f801d929207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch  import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42833a-ccf9-44e8-9356-19933867a294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an untrained dummy model, just to show how saving and loading of weights work\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(15*40,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cd22c-d6cd-414e-bb05-6a59f1b1eee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training, tuning, validation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b237a-e87b-418c-b5ce-a37f37fd606c",
   "metadata": {},
   "source": [
    "Once your model is ready, use the following steps:\n",
    "1. transfer the model to CPU\n",
    "1. grab the learned weights\n",
    "1. save the weights to file\n",
    "1. transfer the file with the weight to the Bot\n",
    "1. On the bot, use the exact same code to construct a model with the same structure\n",
    "1. Load the trained parameters from teh file\n",
    "1. insert the trained parameters into the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ae581-07e5-4144-889c-a86c3760a677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.cpu()    #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964409d-a5f7-41b3-b5db-b55f9df38850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.state_dict()  #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52e4c2-945e-4e21-bd22-a27d04ee21c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(weights, 'model_weights') #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f759f-f9c8-402e-958f-d10e1ed7e9cb",
   "metadata": {},
   "source": [
    "# #4\n",
    "\n",
    "To transfer the weights file to the bot, you will have do that from the bot. The bot can see the gpu server and not the other way around. You will need to do this from a Terminal, because you will need to type in your password.\n",
    "\n",
    "scp jeroen@gpuserver.hhs.nl:notebooks/ds3/model_weights .\n",
    "\n",
    "And af course, you need to use your username instead of 'jeroen' and use the path and filename that corresponds to where the file is. You can replace the . with a foldername to transfer the weights file to that folder.\n",
    "\n",
    "Then on the bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636863aa-1dbf-408a-9891-233e65a87817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an untrained dummy model, just to show how saving and loading of weights work\n",
    "\n",
    "model = nn.Sequential(    #5\n",
    "    nn.Linear(15*40,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3736478-8751-4745-803b-d44eb5621143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.load('model_weights')   #6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf9e18-2afb-450e-a3cc-e2418dd1af7f",
   "metadata": {},
   "source": [
    "Note that this only works if the model has the exact same names and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03122929-8fff-47c6-aa97-b9a745ee16ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(weights)    #7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c241ad9-77c5-447c-a572-011a63997aac",
   "metadata": {},
   "source": [
    "# Inferencing on images\n",
    "\n",
    "Now that you have transfered your model to the model, you will use that on images from the camera. To do this, it is important to prepare the images in the exact same way as you did when validating the images. For example, if you turned the images into grayscale, cropped off the top part and resized the images, you should use the same transformation here. You typically do NOT use any augmentations you have used on the trainingset to learn a more robust model (like random crops, rotations and flips).\n",
    "\n",
    "One other important thing is that you must match the dimensionality of the data that was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4409a-3d50-42cb-972d-eeca5f88fc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run car.ipynb  # this will not run on the gpu server, and you can also use your own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fd47f-f258-4a62-ad59-df7c4efb9880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2890c-ed8d-4647-9b55-4af54c351481",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Car.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f392f-f853-4d2b-a44d-d4eb32999ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_array = c.new_image()    # grab an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ebe48-e1ce-474d-87a0-ab6863165be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.fromarray(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525da6a-d18b-4aae-93cd-4cd7940fedb7",
   "metadata": {},
   "source": [
    "Because you cannot run the car-code on the gpu server, the code below will load an image from a file (the server has no camera) to demonstrate how image preparation for inferencing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5bcdd-8fe7-45b8-a215-ce0c149ca471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_pil = Image.open('angle90/0000-0.02--0.5625.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c3eec-8616-44e2-bdd8-04d5e94c85ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([   \n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(30),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc130a-9bac-4ead-a5af-10311923b8f5",
   "metadata": {},
   "source": [
    "When you check the shape of your image tensor, it will probably be (1,30,40) if the aspect ratio is preserved. The numbers can vary (e.g. if you do not use Grayscale or use other numbers for resize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4f7e4-c967-4bc9-8dbc-dd0ee9a489bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_tensor = transformations(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67633c-52b7-4e7d-8107-a64cc733b68b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885821d-8152-4544-a770-51202484927c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563fc88-625d-4d4f-a0ff-8b7a1eb16ddb",
   "metadata": {},
   "source": [
    "You could do cropping inside or outside of the transformation. Inside the transformation, you have to use transforms.Lamba() with a function in which you crop the image (or write a cutsom transformation class). Outside the transformation, you can simply slice the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b88d93-22fa-4472-a131-aabf2f474c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_tensor = image_tensor[:,15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6e0c0-2bcb-4c6f-8882-72ef0b885bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24460ae2-0d7b-4b27-8fa9-a27dd3de43ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48085f5-a62c-4b83-9ec6-902962a5b436",
   "metadata": {},
   "source": [
    "Now you have to match the dimensionality of the training data. In training, the input tensor consists of multiple images and the first dimension indicates the image number. Here we can add that dimension with an unsqueeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c6a6e-b30f-4558-bcf3-20a2bd698738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c1cdc-269b-4557-a7b9-5ac3bbea565c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b72730-0e87-4829-9d96-c54fa3ec38a4",
   "metadata": {},
   "source": [
    "And now your image is ready to use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecf778-4830-423e-bc70-94cda32ff584",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738aae-256d-45ee-a8f5-7b25d34f91e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
